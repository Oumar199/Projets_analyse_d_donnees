---
title: "heart_attack"
author: "Oumar Kane"
date: "15/03/2022"
output:
  html_document: default
  pdf_document: default
---


# Variables selection with heart attack dataset

We have recuperated a dataset named heart_attack which is very interesting for a debut.


```{r setup, include=FALSE}
library(MASS)
library(randomForest)
library(ggplot2)
library(VSURF)
library(corrplot)
library(VGAM)
library(visdat)
library(ROCR)


```

## We can recuperate the dataset by indicating the path toward it.

```{r}
heart_data_original = read.csv("E:/Oumar/Ordinateur Dell/oumar/documents/Cours/IA data forest/master semestre 2/Analyse de données/Projets/Data/Heart_disease/data/heart2.csv")


# Let's copy the original dataset
heart_data = data.frame(heart_data_original)
```

## Exploration

We have to explore the data set and see how we can do perfectly the data processes. 

Let's attach the dataset to manipulate the columns.
```{r}
attach(heart_data)
```


We have to see the five first lines of the dataset and verify the types.
```{r}
head(heart_data)
```

The data is already clean. We have some categorical variables, but those variables are encoded.
Let's profile deeper the dataset.

## Let's see if the different variables are correlated together with a pair plot and a corrplot.
```{r}
pairs(heart_data)
corrplot(cor(heart_data))
```

The plots traced between two variables are not distinct if we use the whole dataset. But we can do it again with only a few variables.
```{r}
# choose only four first variables
pairs(heart_data[, 1:4])
```

We see that, with the categorical variables, we did not obtain good distributions. We can't interpret the plots with the categorical variables but only with the quantitative variables. Let's recuperate only the quantitative variables in a new data frame.
```{r}
quanti = heart_data[, c(1, 4, 5, 8, 10)]
```


Let's trace a new pair plot without categorical variables.
```{r}
pairs(quanti)
```

We can see that some variables like thalach and age or chol and age are correlated.
```{r}
# Let's trace the corrplot to see more clearly the variables interactions
corrplot(cor(quanti))
```

**Conclusion of corrplot:** We see that many variables are correlated together. We see correlations between :

- The age variable and the other quantitative variables;

- oldpeak, trestbps and thalach

- chol and trestbps.

### Abnormal values

We must change the type of the target to factor at first.

```{r}
heart_data[,14] = as.factor(heart_data[,14])
```

Let's verify if the data set contains abnormal values with box plots.

We can create a function that we will use for tracing box plot for each variable of the data set. We can add value limitations to the boxplots. 
```{r}
trace_boxplots = function(variables, data, quanti_data = NULL)
  for(i in 1:length(variables)){
    # Recuperate the data set
    if(is.null(quanti_data)){
      quanti_data = data
    }
    
    ## Calculate the limitations
    # quantile 0.25
    q1 = quantile(quanti_data[, i], 0.25)
    
    # quantile 0.75
    q2 = quantile(quanti_data[, i], 0.75)
    
    # interquartile
    inter_q = q2 - q1
    
    # high limit
    high_limit = q2 + 1.5*inter_q
    
    # low limit
    low_limit = q1 - 1.5*inter_q
    
    # Get the variable
    variable = variables[i]
    
    # create the graphic
    plot = ggplot(data = data, aes_string("target", variable, fill = "target")) +
      geom_boxplot() + 
      ggtitle(paste("Boxplot of", variable)) +
      geom_hline(yintercept = high_limit, color = "red", linetype = "dashed") +
      geom_hline(yintercept = low_limit, color = "red", linetype = "dashed") +
      geom_text(aes(0.5, high_limit+.2, label = "high limit", color = "red"), size = 3) +
      geom_text(aes(0.5, low_limit+.2, label = "low limit", color = "red"), size = 3) +
      theme(plot.title = element_text(hjust = 0.5))
    
    # Show out the plot
    print(plot)
  }  

# Recuperate the quantitative variables' names.
quanti_variables = names(quanti)

# Let's use, now, the function to trace the box plots.
trace_boxplots(quanti_variables, heart_data, quanti)
```

We can see, with limitations, some abnormal values on boxplots but in a low quantity. We can delete all abnormal values. But, for the moment, we can maintain them and go deeper into the exploration.

### na values
Let's check if the data set contains missing values with a heatmap.
```{r}
vis_miss(heart_data)
```

The data set doesn't contain any missing values. Very good !!

We can begin the processing.

## Preprocessing

### Let's change the type of the categorical variables to factor

```{r}
for (i in c(2, 3, 6, 7, 9, 11, 12, 13)){
  heart_data[, i] = as.factor(heart_data[, i])
}
```


### Let's see if we obtain a good fit with a logistic regression model
We use a logistic regression model because the target `target` is a binary categorical variable.
```{r}
summary(glm(target~., data = heart_data, family = binomial))
```

Some variables' p values are over 0.05. So they don't add much information to the model. 

## We can begin the selection using the likelihood ratio

```{r}
# create a function calculating the likelihood ratio
LRT_dev = function(mod1, mod2){
  p_value =  1 - pchisq(deviance(mod1) - deviance(mod2), df.residual(mod1) - df.residual(mod2)) 
  result = p_value < 0.05
  print(paste("p_value is : ", p_value))
  print(paste("p_value < 0.05 : ", as.logical(p_value < 0.05)))
  return(result)
}
```


## Forward selection with likelihood ratio

===========================================================================================

### Let's add a function that gives us a formula automatically 
```{r}
get_formula_add = function(target, variables){
 return(as.formula(                      # Create formula
  paste(paste(target, " ~ "), paste(variables, collapse = " + "))))
}
get_formula_rem = function(target, variables, all_variables){
 return(as.formula(                      # Create formula
  paste(paste(target, " ~ "), paste(paste(paste(all_variables, collapse = " + "), " - ")), paste(variables, collapse = " - "))))
}
```


### We can recuperate in a variable the names of the columns to test our function
```{r}
variables = names(heart_data)[-length(heart_data)]
glm(get_formula_add("target", variables), data = heart_data, family = binomial)
```

Our function goes well as we expected it.

### Let's made a function that performs the LRT analysis directly. 
```{r}
process_LRT = function(target, variables, data, model, family, mode = "forward"){
  # We initialize the model to NULL.
  mod = NULL
  
  # if the chosen mode is forward we will process a forward selection.
  if(mode == "forward"){
    
    # print out the title.
    print("Forward Selection :")
    print("--------------------------")
    
    # Let's get a model which uses as a feature a constant (that's the current model).
    mod = model(target~1, data = data, family = family)
    
    # We initialize an empty list that will stock the features to add to the model (we include those features).
    testing_variables = c()
    
    # We initialize the index of the first feature to add in testing_variables.
    j = 1
    
    # We iterate on each index of the list of variables to test.
    for(i in 1:length(variables)){
      
      # Add in testing_variables a new variable to test.
      testing_variables[j] = variables[i]
      
      # Include testing features in a new model.
      new_model = model(get_formula_add(target, testing_variables), data = data, family = family)
      
      # Print out the name of the variable to test
      print(paste("Variable ", variables[i]))
      
      # We verify if the variable to test is relevant or not with the LRT analysis.
      if(LRT_dev(mod, new_model) == 1){
        # If the variable is relevant
        # We increment the length of the testing variables to 1 for the following variable to test.
        j = j + 1
        
        # We recuperate the current model as the new model.
        mod = new_model
        
        # We can print out that the last tested variable is relevant.
        print(paste("The variable ", paste(variables[i], " is relevant")))
      }
      else{
        # If the variable is not relevant
        # We print out that the last tested variable is not relevant.
        print(paste("The variable ", paste(variables[i], " is not relevant")))
      }
      
      # Separate tests.
      print("========================================================================================================")
    }
  }
  # if the chosen mode is backward we will process a backward selection.
  else if(mode == "backward"){
    # print out the title.
    print("Backward Selection :")
    print("--------------------------")
    
    # Let's include all features in the current model.
    mod = model(get_formula_add(target, variables), data = data, family = family)
    
    # We initialize an empty list that will stock the features to remove from the model.
    testing_variables = c()
    
    # We initialize the index of the first feature to add in testing_variables.
    j = 1
    
    # We iterate on each index of the list of variables to test.
    for(i in 1:length(variables)){
      
      # Add in testing_variables a new variable to test.
      testing_variables[j] = variables[i]
      
      # Create a new model which does not include the testing features.
      new_model = model(get_formula_rem(target, testing_variables, variables), data = data, family = family)
      
      # Print out the name of the variable to test
      print(paste("Variable ", variables[i]))
      
      # We verify if the variable to test is relevant or not with the LRT analysis.
      if(LRT_dev(new_model, mod) == 0){
        # If the variable is not relevant
        # We increment the length of the testing variables to 1 for the following variable to test.
        j = j + 1
        
        # We recuperate the current model as the new model.
        mod = new_model
        
        # We can print out that the last tested variable is not relevant.
        print(paste("The variable ", paste(variables[i], " is not relevant")))
      }
      else{
        # If the variable is relevant
        # We print out that the last tested variable is relevant. So we don't remove the variable from the model.
        print(paste("The variable ", paste(variables[i], " is relevant")))
      }
      # Separate tests.
      print("========================================================================================================")
    }
  }
  # If the chosen mode doesn't exist, we raise an error.
  else{
    print(paste(paste("Error ! The choice ", mode), " doesn't exist"))
  }
  # For the forward, or the backward, we will finally return the final mode that contains the selected variables.
  return(mod)
}

```

### We can process the forward selection with the created function.
```{r}
model_final_forw = process_LRT("target", variables, heart_data, glm, binomial, "forward")
```


### Conclusion : 
The final model doesn't contain the fbs variable.

```{r}
# summary of the model
summary(model_final_forw)
```


### Let's process now the backward selection.
```{r}
model_final_back = process_LRT("target", variables, heart_data, glm, binomial, "backward")
```

### conclusion : 
The final model doesn't contain the following variables : age, fbs and restecg.
```{r}
# summary of the model
summary(model_final_back)
```


## Let's make variables selection with the stepwise trick
The stepwise function (stepAIC) takes, as main parameters, the model (logistic) and
the direction that can be forward, backward, or both. We can choose both directions
to obtain the best possible model. The time complexity of
the stepwise method is very high but we will finally obtain a good selection.
```{r}
model = glm(target~., data = heart_data, family = binomial)
step.model = stepAIC(model, direction = "both")
```

```{r}
# summary of the stepwise model
summary(step.model)
```
### Conclusion of the stepwise selection : 
The variables that the model decided to use for his training are age, sex, cp, trestbps, chol, thalach, exang, oldpeak, slope, ca, and thal. So only the fbs and restecg variables didn't be chosen.

## Make a selection with the random forest classification model :
This method selects the best variables by using the random forest classification model. The random forest model is known to be a non-parametrical model.
```{r}
forest.model = randomForest(target~., data = heart_data, importance = TRUE)
```

```{r}
summary(forest.model)
```

```{r}
# plot the OOB errors
plot(forest.model)
```

Here we see that we can obtain a good fit only with 90 trees.

Let's make another random forest model with only 90 trees.

```{r}
forest.model = randomForest(target~., data = heart_data, importance = TRUE, ntrees = 90)
```

### Let's plot the classification of the variables
```{r}
varImpPlot(forest.model, type = 1, c = 1, pch = 20, cex = 1, main = "Random Forest selection")
```

**The most relevant variables are the caa, thall, sex, cp, oldpeak and thalachh variables that we previously selected with the other methods.**

With the VSURF package we will interprate the selection and visualize the best choices.
```{r}
y = heart_data[, 14]
x = heart_data[, -14]
th1 = VSURF_thres(x, y, ntree = 90)
```

```{r}
vs1 = VSURF_interp(x, y, vars = th1$varselect.thres)
```

Let's visualize the number of choosed variables 
```{r}
plot(vs1)
```

The model chose the 7 best variables finally (the number can change over the training we can choose a seed to fix it). Let's print out the names of those variables. 
```{r}
names(x)[vs1$varselect.interp]
```
With the randomForest model, the following variables are not relevant: sex, slope, exang, restecg, trestbps, and fbs.


**Conclusion:** With the random forest we found many not relevant variables and with the stepwise, the not relevant variables were fbs and restecg. We must, after that, verify if all those variables are actually not relevant with the help of the LRT analysis.


We created a function that can process the likelihood ratio on a full data set. But we can also create a function that can process the likelihood ratio on a few variables only. We must, before that, maintain some variables as relevant and we will investigate, after that, the other variables to check if they are relevant or not. Let's create the function and test it with the not relevant variables.

## Match function. 
The match function will be made like the process LRT function but with different arguments

```{r}
match_variables_LRT = function(target, relevant_variables, variables_to_check, data, model, family){
  print("Let's check if the provided variables are important :")
  print("--------------------------")
  mod = model(get_formula_add(target, relevant_variables), data = data, family = family)
  testing_variables = relevant_variables
  j = length(relevant_variables) + 1
  for(i in 1:length(variables_to_check)){
      testing_variables[j] = variables_to_check[i]
      new_model = model(get_formula_add(target, testing_variables), data = data, family = family)
      print(paste("Variable ", variables_to_check[i]))
      if(LRT_dev(mod, new_model) == 1){
        print(paste("The variable ", paste(variables_to_check[i], " is relevant")))
      }
      else{
        print(paste("The variable ", paste(variables_to_check[i], " is not relevant")))
      }
      print("========================================================================================================")
  }
}
```

## Test some variables with the match function
The variables to test with the match function are listed as following : 'sex', 'slope', 'exang', 'restecg', 'trestbps', 'fbs'. Another variables will be considerate as relevant.
```{r}
# recuperate before that the relevant variables 
variables_to_exclude = c('target', 'sex', 'slope', 'exang', 'restecg', 'trestbps', 'fbs')
relevant_variables = names(heart_data[,-which(names(heart_data) %in% variables_to_exclude)])
variables_to_check = variables_to_exclude[-1]
match_variables_LRT("target", relevant_variables, variables_to_check, heart_data, glm, "binomial")
```

**Conclusion**: The not relevant variable will be finally: fbs. We can create the final model which will contain only the relevant variables.

### final model
```{r}
# update the relevant variables
variables_to_exclude = c('target', 'fbs')
relevant_variables = names(heart_data[,-which(names(heart_data) %in% variables_to_exclude)])
final_model = glm(get_formula_add("target", relevant_variables), data = heart_data, family = "binomial")
print(final_model)
```


We got a final model but we must split the dataset into a training set et a testing set if we want to make good predictions.

## Sampling: Learning vs Testing

We can now try to split the dataset into training set and testing set.
```{r}
heart_data[, 1:13] = heart_data[, which(names(heart_data) %in% relevant_variables)]
set.seed(0)
d = sort(sample(nrow(heart_data), nrow(heart_data) * 0.7))
```

Train sample
```{r}
appren <- heart_data[d, ]
summary(appren)
```
Test sample

```{r}
test <- heart_data[-d, ]
summary(test)
```
## Modelization

The model used here is the logistic regression which can be found in R as glm.

We introduce all the variables in our model except fbs
```{r}
get_formula_add("target", relevant_variables)
```
```{r}
m.logit <- glm(get_formula_add("target", relevant_variables), data = appren, family = binomial)

summary(m.logit)
```
## Model validation: Quality and robustness indicators

After obtaining a model, it is necessary to diagnose the regression in order to validate or not the model.

```{r}
par(mfrow = c(1, 1))
plot(rstudent(m.logit), type = "p", cex = 0.5, ylab = "Résidus studentisés ", 
    col = "springgreen2", ylim = c(-3, 3))
abline(h = c(-2, 2), col = "red")
```
```{r}
(chi2 <- with(m.logit, null.deviance - deviance))
```


```{r}
(ddl <- with(m.logit, df.null - df.residual))
```


```{r}
(pvalue <- pchisq(chi2, ddl, lower.tail = F))
```

**Conclusion**: The p-value is very low. So we can say that the model is efficient.

## We are now going to try to validate on the test sample that we have previously defined

Here are the steps we will follow to validate our model

On the training sample and on the test sample:

We calculate a confusion matrix: and therefore we measure an error rate
the air is evaluated under the ROC curve
```{r}
appren.p <- cbind(appren, predict(m.logit, newdata = appren, type = "link", 
    se = TRUE))
head(appren.p)
appren.p <- within(appren.p, {
    PredictedProb <- plogis(fit)
    LL <- plogis(fit - (1.96 * se.fit))
    UL <- plogis(fit + (1.96 * se.fit))
})
tail(appren.p)
```
```{r}
appren.p <- cbind(appren.p, pred.target = factor(ifelse(appren.p$PredictedProb > 
    0.5, 1, 0)))
head(appren.p)
```
```{r}
(m.confusion <- as.matrix(table(appren.p$pred.target, appren.p$target)))
```


```{r}
m.confusion <- unclass(m.confusion)
# Taux d'erreur
Tx_err <- function(y, ypred) {
    mc <- table(y, ypred)
    error <- (mc[1, 2] + mc[2, 1])/sum(mc)
    print(error)
}
Tx_err(appren.p$pred.target, appren.p$target)
```
### calculation of the error rate on the training sample

```{r}
test.p <- cbind(test, predict(m.logit, newdata = test, type = "response", se = TRUE))
test.p <- cbind(test.p, pred.target <- factor(ifelse(test.p$fit > 0.5, 1, 0)))
(m.confusiontest <- as.matrix(table(test.p$pred.target, test.p$target)))

```
### calculation of the error rate on the test sample

```{r}
m.confusiontest <- unclass(m.confusiontest)

Tx_err(test.p$pred.target, test.p$target)
```

We obtain almost zero.

### Construction of ROC curves: 
This curve, or rather the area under it, represents the sensitivity/specificity of the model. A model is good if positives (1) were predicted positives and 0 were predicted 0.
Generally, we are interested in both the shape of the curve and the area under it: 1–> Ideal model, 0.5 –> Random model;

Principle of the ROC curve: if the test gives a numerical result with a threshold t such that the prediction is positive if x > t, and the prediction is negative if x < t, then as t increases:

the specificity increases.
but the sensitivity decreases.
The ROC curve represents the change in sensitivity (rate of true positives) as a function of 1 - specificity (rate of false positives) when the threshold t is varied.

It is an increasing curve between the point (0,0) and the point (1, 1) and in principle above the first bisector. A random prediction would give the first bisector. The better the prediction, the more the curve is above the first bisector. An ideal prediction is the horizontal y=1 on ]0,1] and the point (0,0). The area under the ROC curve (AUC, Area Under the Curve) gives an indicator of the quality of the prediction (1 for an ideal prediction, 0.5 for a random prediction).
```{r}
Pred = prediction(appren.p$PredictedProb, appren.p$target)
Perf = performance(Pred, "tpr", "fpr")
plot(Perf, colorize = TRUE, main = "ROC apprentissage")
```
```{r}
perf <- performance(Pred, "auc")
perf@y.values[[1]]
```

To get the area under the curve, we will rather use
```{r}
Predtest = prediction(test.p$fit, test.p$target)
Perftest = performance(Predtest, "tpr", "fpr")
perftest <- performance(Predtest, "auc")
perftest@y.values[[1]]
```

We will make the roc curve with each set (train and test) and we will verify if the model is overfitted.

```{r}
par(mfrow = c(1, 2))
plot(Perf, colorize = TRUE, main = "Training ROC - AUC= 0.94")
plot(Perftest, colorize = TRUE, main = "Testing ROC - AUC = 0.94 ")
```
**conclusion**: Both train and test roc curves indicate that the model fits well (very good !!). We see also that we didn't obtain overfitting because the difference between the two areas under curves is close to 0.

